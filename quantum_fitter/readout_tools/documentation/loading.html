<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>quantum_fitter.readout_tools.loading API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:20%;max-height:5em;margin:auto;margin-bottom:.3em}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>readout_tools.loading</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import matplotlib.pyplot as plt
import Labber as lab
import numpy as np

class DataImport:
    &#34;&#34;&#34;This class contains all importing and reformatting functions. Furthermore it controls the data size used and the import and export of classifiers.
    &#34;&#34;&#34;
    def __init__(self, filePath=None, channelName=None, state_entries=None, 
                 labels=None, size=None, kfolds=10):
        &#34;&#34;&#34;Initializes a instance of the class. If no entries are given it defines the best entries for classification from the mean. 

        Args:
            filePath (string, optional): File path for the Labber (h5file) file containing the IQ data. Defaults to None.
            channelName (string, optional): Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            labels (list, optional): A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.
            size (int, optional): The size of the data set used. Must be integer. Defaults to None.
            kfolds (int, optional): Number of splits in the dataset. Used for crossvalidation. Defaults to 10.
        &#34;&#34;&#34;
        
        self._filePath = filePath
        self.h5file = lab.LogFile(self._filePath)
        self.channeldict = self.h5file.getChannelValuesAsDict()
        
        self.size = size
        self.kfolds = kfolds
        
        self.kernel = None
        self.classifer = None
        self.set_data(channelName=channelName)
    
        # define states
        self.data_mean = self._min_max_index()
    
        if state_entries:
            self.state_entries = state_entries
        else:
            self.state_entries = self.data_mean[1]
          
        self.set_states(state_entries=self.state_entries, labels=labels)  
            
    def set_data(self, channelName=None, state_entries=None, channelName_log=&#39;Pulse Generator - Amplitude&#39;, unit_log=&#39;V&#39;):
        &#34;&#34;&#34;Selects the data set which is gonna be used in the further calculations.

        Args:
            channelName (string, optional): Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            channelName_log (str, optional): The units scale to be used instead of index. Defaults to &#39;Pulse Generator - Amplitude&#39;.
            unit_log (str, optional): The units of the unit scale. Defaults to &#39;V&#39;.
        &#34;&#34;&#34;
        import h5py
        
        h5data = self.h5file.getData(channelName, entry=state_entries)
        self.h5data = self._reformate(h5data)
        
        self.h5data_index = {&#39;name&#39; : &#39;Index&#39;, 
                             &#39;axis&#39; : range(self.h5file.getNumberOfEntries())}
        
        try:
            h5data_temp = h5py.File(self._filePath, &#39;r&#39;)
            h5datalog = h5data_temp[&#39;Data&#39;][&#39;Data&#39;][:]
            self.h5data_log = {&#39;name&#39; : f&#39;{channelName_log} [{unit_log}]&#39;, 
                               &#39;axis&#39; : h5datalog[:,0,0]}
        except:
            print(&#39;channelName_log not in h5data. Setting Pulse Generator Amplitude to index&#39;)
            self.h5data_log = self.h5data_index
             
    def set_states(self, state_entries=None, labels=None, offset=None):
        &#34;&#34;&#34;Sets the entries of the &#34;cleanest&#34; states of the data set to be used in further calculations.

        Args:
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            labels (list, optional): A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.
            offset (float), optional): The offset value offsets the last state in the array. This argument is added to simulate three state data. Defaults to None.
        &#34;&#34;&#34;
        if state_entries:
            self.state_entries = state_entries
        
        self._int_states = self.h5data[np.array(self.state_entries)]
        
        if offset:
            self._int_states[-1] = self._int_states[-1] + offset
        
        if labels != None: 
                self._states_labels = labels 
        else: 
            self._states_labels = range(len(self.state_entries))
        
        self.set_dataset_size()
                 
    def set_dataset_size(self, size=None, X=None):
        &#34;&#34;&#34;Sets the size of all entries in the data set.

        Args:
            size (int, optional): The size of the data set used. Must be integer. Defaults to None.
            X (list, optional): A list of data. If None the selected initial states are used. Defaults to None.

        Returns:
            List: Dataset shortened by size. (X[:size])
        &#34;&#34;&#34;
        if size:
            self.size = int(size)
         
        X_temp = False    
        if X is None:
            X_temp = True
            X = self._int_states
            
        state_list, label_list  = [], []
        for i, j in enumerate(X):
            state_list.append(j[:self.size])
            if X_temp == True: 
                label_list.append(np.ones(state_list[i].shape[0]) * self._states_labels[i])
            
        if X_temp == True:  
            self._states_X = np.concatenate(state_list)
            self._states_target = np.concatenate(label_list)
        else:
            return state_list

        self._split_data()
    
    def _split_data(self):
        &#34;&#34;&#34;Split dataset of initial states it into training and testing parts. The size of the test sets will be 1/k_folds, where k_folds = 10 as default.
        &#34;&#34;&#34;
        from sklearn.model_selection import train_test_split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self._states_X, self._states_target, 
                                                            test_size=(1/self.kfolds), random_state=0)
        
        self._matrix_weights()
        
    def _reformate(self, X):
        &#34;&#34;&#34;A function that reformates data if not in the right format. The wanted format is [[i,q],[i,q], ...]
        &#34;&#34;&#34; 
        h5data_reformated = []
        for i in range(len(X)):
            h5data_reformated.append(np.column_stack((X[i].real, X[i].imag)))
        return np.array(h5data_reformated)
        
    def _min_max_index(self, X=None):
        &#34;&#34;&#34;Max/min function used to determine the initial states, by calculating the mean of the data in the IQ plane for all entries and Finding the index for the min and max value.

        Args:
            X (list, optional): The IQ data to examinen. If None the default data is used: self.h5data. Defaults to None.

        Returns:
            Data_mean (float): The meaning of all the data entries.
            Max/min (list): A list containing the min and max index. Formate: [min, max]
        &#34;&#34;&#34;
        if X is None:
            X = self.h5data
            
        data_mean = np.linalg.norm(X, axis=2)
        data_mean = np.sum(data_mean, axis=1)
        
        min_, max_ = np.argmin(data_mean), np.argmax(data_mean)
        return data_mean, [min_, max_]
    
    def _matrix_weights(self, X=None, y=None, A=0.1, B=1, bins=100, r=5, plot=False):
        &#34;&#34;&#34;Calculate a waited matrix for the initial states. This is used to optimize the wall clock time of the calculation.

        Args:
            X (list, optional): The X-data to use. If None the Initial states are used: self.X_train. Defaults to None.
            y (list, optional): The y-data to use. If None the Initial states are used: self.y_train. Defaults to None.
            A (float, optional): The minimum of the final normalization of the array. Defaults to 0.1.
            B (float, optional): The maximum of the final normalization of the array. Defaults to 1.
            bins (int, optional): Number of bins in the generated matrix. This is a measure of resolution. Defaults to 100.
            r (int, optional): Number of beans to some in the calculation. This helps to smooth loud the matrix. Defaults to 5.
            plot (bool, optional): If True matrices are plotted. Defaults to False.
        &#34;&#34;&#34;
        def matrix_avg(M, r=1, A=0, B=1): 
            M_sum = np.full(M.shape, 0.0)
           
            for i in range(M.shape[0]):
                for j in range(M.shape[1]):
                    n0 = n1 = r
                        
                    if (i-n0) &lt;= -1: 
                        n0 = i
                        
                    if (j-n1) &lt;= -1: 
                        n1 = j  
                
                    Mi = M[i-n0:i+r+1, j-n1:j+r+1]
                
                    M_sum[i,j] = np.sum(Mi) / Mi.size
            M_sum = np.interp(M_sum, (M_sum.min(), M_sum.max()), (A, B))
            return M_sum 
        
        if (X is None):
            X = self.X_train
        
        if (y is None):
            y = self.y_train
        
        xy = [X[y == label] for label in np.unique(y)]
        
        x, y = [], []
        for i in range(len(xy)):
            x.append(xy[i][:, 0])
            y.append(xy[i][:, 1])

        x_con, y_con = np.concatenate(x), np.concatenate(y)
        lim = [[min(x_con), max(x_con)], [min(y_con), max(y_con)]]

        data_matrix = []
            
        for i in range(len(xy)):
            data_matrix_i, xaxis, yaxis = np.histogram2d(x[i], y[i], bins=bins, range=lim)
            data_matrix_i = matrix_avg(data_matrix_i, r=r)
            data_matrix.append(data_matrix_i)
        
        background_matrix = np.minimum.reduce(data_matrix)
        
        if plot == True:
            plt.matshow(background_matrix, cmap=plt.cm.viridis)
            plt.show()
            
        weights = np.empty(0)
        for i in range(len(xy)):
            data_matrix[i] = data_matrix[i] - background_matrix
            if plot == True:
                plt.matshow(data_matrix[i], cmap=plt.cm.viridis)
                plt.show()
                
            weights_i = np.empty(0)
            for j in range(len(xy[i])):
                    index_x = np.absolute(xaxis - xy[i][j, 0]).argmin()
                    index_y = np.absolute(yaxis - xy[i][j, 1]).argmin()

                    weights_i = np.append(weights_i, data_matrix[i][index_x - 1, index_y - 1])

            weights = np.hstack([weights, weights_i])
        self.weights = np.interp(weights, (weights.min(), weights.max()), (A, B))

    def export_classifier(self, filePath=None, fileName=None, fileSave=&#39;all&#39;, overwrite=True):
        &#34;&#34;&#34;A function to export the classifier after being fitted or defined. It uses the pickle format, and prints a statement if successfully saved: &#34;Saved the pickle&#34;.

        Args:
            filePath (string, optional): The file path to safe to pickle file in. If None the datafile path is used. Defaults to None.
            fileName (string, optional): The filename of the exported file. If None the datafilename is used, but as a .pickle file. Defaults to None.
            fileSave (string, optional): The part to save. If &#39;all&#39; the entire self saved. Defaults to &#39;all&#39;.
            overwrite (bool, optional): If False And a pickle file with the same name is in the folder a counter (_000) is added to the name. . Defaults to True.
        &#34;&#34;&#34;
        import pickle
        import os.path

        # save type
        if fileSave == &#39;classifier&#39;:
            fileSave = self.cv_search
            
        elif fileSave == &#39;all&#39; or fileSave == None:
            fileSave = self

        else:
            fileSave = self.fileSave
        
        # save path
        if filePath == None:
            filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
            
        if filePath.endswith(&#39;/&#39;) == False:
            filePath += &#39;/&#39;
    
        # save name
        if fileName == None:
            fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)
    
        # combine
        file = filePath + fileName
        
        # adds number
        if overwrite == False:
            i = 0
            file_temp = file
            while os.path.exists(file + &#39;.pickle&#39;) == True:
                file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
                i += 1
            
        with open(filePath + fileName + &#39;.pickle&#39;, &#39;wb&#39;) as handle:
            pickle.dump(fileSave, handle, protocol=pickle.HIGHEST_PROTOCOL)
        
        print(&#39;Saved the pickle&#39;)
    
    def import_classifier(self, filePath=None):
        &#34;&#34;&#34;A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: &#34;Got your pickle!&#34;. It&#39;s not successful: &#34;Unable to locate your pickle&#34;.

        Args:
            filePath (string, optional): The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.

        Returns:
            Classifier (self): The classifier object
        &#34;&#34;&#34;
        import pickle
        import os.path
        
        try: 
            filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
            fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)
            
            if filePath.endswith(&#39;/&#39;) == False:
                filePath += &#39;/&#39;
            
            # combine
            file = filePath + fileName
            
            i = 999
            file_temp = file
            while os.path.exists(file + &#39;.pickle&#39;) == False and i&gt;0:
                
                file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
                i -= 1
            
        
            with open(file + &#39;.pickle&#39;, &#39;rb&#39;) as handle:
                classifier = pickle.load(handle)
            
            print(&#39;Got your pickle!&#39;)
            return classifier
        
        except:
            print(&#39;Unable to locate your pickle&#39;)

def import_classifier(filePath):
    &#34;&#34;&#34;A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: &#34;Got your pickle!&#34;. It&#39;s not successful: &#34;Unable to locate your pickle&#34;. This can be used without a instance.

        Args:
            filePath (string, optional): The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.

        Returns:
            Classifier (self): The classifier object
        &#34;&#34;&#34;
    import pickle
    
    try: 
        with open(filePath, &#39;rb&#39;) as handle:
            classifier = pickle.load(handle)
        
        print(&#39;Got your pickle!&#39;)
        return classifier
    
    except:
        print(&#39;Unable to locate your pickle&#39;)
        
     </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="quantum_fitter.readout_tools.loading.import_classifier"><code class="name flex">
<span>def <span class="ident">import_classifier</span></span>(<span>filePath)</span>
</code></dt>
<dd>
<div class="desc"><p>A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: "Got your pickle!". It's not successful: "Unable to locate your pickle". This can be used without a instance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filePath</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Classifier (self): The classifier object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_classifier(filePath):
    &#34;&#34;&#34;A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: &#34;Got your pickle!&#34;. It&#39;s not successful: &#34;Unable to locate your pickle&#34;. This can be used without a instance.

        Args:
            filePath (string, optional): The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.

        Returns:
            Classifier (self): The classifier object
        &#34;&#34;&#34;
    import pickle
    
    try: 
        with open(filePath, &#39;rb&#39;) as handle:
            classifier = pickle.load(handle)
        
        print(&#39;Got your pickle!&#39;)
        return classifier
    
    except:
        print(&#39;Unable to locate your pickle&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="quantum_fitter.readout_tools.loading.DataImport"><code class="flex name class">
<span>class <span class="ident">DataImport</span></span>
<span>(</span><span>filePath=None, channelName=None, state_entries=None, labels=None, size=None, kfolds=10)</span>
</code></dt>
<dd>
<div class="desc"><p>This class contains all importing and reformatting functions. Furthermore it controls the data size used and the import and export of classifiers.</p>
<p>Initializes a instance of the class. If no entries are given it defines the best entries for classification from the mean. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filePath</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>File path for the Labber (h5file) file containing the IQ data. Defaults to None.</dd>
<dt><strong><code>channelName</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.</dd>
<dt><strong><code>state_entries</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The size of the data set used. Must be integer. Defaults to None.</dd>
<dt><strong><code>kfolds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of splits in the dataset. Used for crossvalidation. Defaults to 10.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataImport:
    &#34;&#34;&#34;This class contains all importing and reformatting functions. Furthermore it controls the data size used and the import and export of classifiers.
    &#34;&#34;&#34;
    def __init__(self, filePath=None, channelName=None, state_entries=None, 
                 labels=None, size=None, kfolds=10):
        &#34;&#34;&#34;Initializes a instance of the class. If no entries are given it defines the best entries for classification from the mean. 

        Args:
            filePath (string, optional): File path for the Labber (h5file) file containing the IQ data. Defaults to None.
            channelName (string, optional): Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            labels (list, optional): A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.
            size (int, optional): The size of the data set used. Must be integer. Defaults to None.
            kfolds (int, optional): Number of splits in the dataset. Used for crossvalidation. Defaults to 10.
        &#34;&#34;&#34;
        
        self._filePath = filePath
        self.h5file = lab.LogFile(self._filePath)
        self.channeldict = self.h5file.getChannelValuesAsDict()
        
        self.size = size
        self.kfolds = kfolds
        
        self.kernel = None
        self.classifer = None
        self.set_data(channelName=channelName)
    
        # define states
        self.data_mean = self._min_max_index()
    
        if state_entries:
            self.state_entries = state_entries
        else:
            self.state_entries = self.data_mean[1]
          
        self.set_states(state_entries=self.state_entries, labels=labels)  
            
    def set_data(self, channelName=None, state_entries=None, channelName_log=&#39;Pulse Generator - Amplitude&#39;, unit_log=&#39;V&#39;):
        &#34;&#34;&#34;Selects the data set which is gonna be used in the further calculations.

        Args:
            channelName (string, optional): Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            channelName_log (str, optional): The units scale to be used instead of index. Defaults to &#39;Pulse Generator - Amplitude&#39;.
            unit_log (str, optional): The units of the unit scale. Defaults to &#39;V&#39;.
        &#34;&#34;&#34;
        import h5py
        
        h5data = self.h5file.getData(channelName, entry=state_entries)
        self.h5data = self._reformate(h5data)
        
        self.h5data_index = {&#39;name&#39; : &#39;Index&#39;, 
                             &#39;axis&#39; : range(self.h5file.getNumberOfEntries())}
        
        try:
            h5data_temp = h5py.File(self._filePath, &#39;r&#39;)
            h5datalog = h5data_temp[&#39;Data&#39;][&#39;Data&#39;][:]
            self.h5data_log = {&#39;name&#39; : f&#39;{channelName_log} [{unit_log}]&#39;, 
                               &#39;axis&#39; : h5datalog[:,0,0]}
        except:
            print(&#39;channelName_log not in h5data. Setting Pulse Generator Amplitude to index&#39;)
            self.h5data_log = self.h5data_index
             
    def set_states(self, state_entries=None, labels=None, offset=None):
        &#34;&#34;&#34;Sets the entries of the &#34;cleanest&#34; states of the data set to be used in further calculations.

        Args:
            state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
            labels (list, optional): A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.
            offset (float), optional): The offset value offsets the last state in the array. This argument is added to simulate three state data. Defaults to None.
        &#34;&#34;&#34;
        if state_entries:
            self.state_entries = state_entries
        
        self._int_states = self.h5data[np.array(self.state_entries)]
        
        if offset:
            self._int_states[-1] = self._int_states[-1] + offset
        
        if labels != None: 
                self._states_labels = labels 
        else: 
            self._states_labels = range(len(self.state_entries))
        
        self.set_dataset_size()
                 
    def set_dataset_size(self, size=None, X=None):
        &#34;&#34;&#34;Sets the size of all entries in the data set.

        Args:
            size (int, optional): The size of the data set used. Must be integer. Defaults to None.
            X (list, optional): A list of data. If None the selected initial states are used. Defaults to None.

        Returns:
            List: Dataset shortened by size. (X[:size])
        &#34;&#34;&#34;
        if size:
            self.size = int(size)
         
        X_temp = False    
        if X is None:
            X_temp = True
            X = self._int_states
            
        state_list, label_list  = [], []
        for i, j in enumerate(X):
            state_list.append(j[:self.size])
            if X_temp == True: 
                label_list.append(np.ones(state_list[i].shape[0]) * self._states_labels[i])
            
        if X_temp == True:  
            self._states_X = np.concatenate(state_list)
            self._states_target = np.concatenate(label_list)
        else:
            return state_list

        self._split_data()
    
    def _split_data(self):
        &#34;&#34;&#34;Split dataset of initial states it into training and testing parts. The size of the test sets will be 1/k_folds, where k_folds = 10 as default.
        &#34;&#34;&#34;
        from sklearn.model_selection import train_test_split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self._states_X, self._states_target, 
                                                            test_size=(1/self.kfolds), random_state=0)
        
        self._matrix_weights()
        
    def _reformate(self, X):
        &#34;&#34;&#34;A function that reformates data if not in the right format. The wanted format is [[i,q],[i,q], ...]
        &#34;&#34;&#34; 
        h5data_reformated = []
        for i in range(len(X)):
            h5data_reformated.append(np.column_stack((X[i].real, X[i].imag)))
        return np.array(h5data_reformated)
        
    def _min_max_index(self, X=None):
        &#34;&#34;&#34;Max/min function used to determine the initial states, by calculating the mean of the data in the IQ plane for all entries and Finding the index for the min and max value.

        Args:
            X (list, optional): The IQ data to examinen. If None the default data is used: self.h5data. Defaults to None.

        Returns:
            Data_mean (float): The meaning of all the data entries.
            Max/min (list): A list containing the min and max index. Formate: [min, max]
        &#34;&#34;&#34;
        if X is None:
            X = self.h5data
            
        data_mean = np.linalg.norm(X, axis=2)
        data_mean = np.sum(data_mean, axis=1)
        
        min_, max_ = np.argmin(data_mean), np.argmax(data_mean)
        return data_mean, [min_, max_]
    
    def _matrix_weights(self, X=None, y=None, A=0.1, B=1, bins=100, r=5, plot=False):
        &#34;&#34;&#34;Calculate a waited matrix for the initial states. This is used to optimize the wall clock time of the calculation.

        Args:
            X (list, optional): The X-data to use. If None the Initial states are used: self.X_train. Defaults to None.
            y (list, optional): The y-data to use. If None the Initial states are used: self.y_train. Defaults to None.
            A (float, optional): The minimum of the final normalization of the array. Defaults to 0.1.
            B (float, optional): The maximum of the final normalization of the array. Defaults to 1.
            bins (int, optional): Number of bins in the generated matrix. This is a measure of resolution. Defaults to 100.
            r (int, optional): Number of beans to some in the calculation. This helps to smooth loud the matrix. Defaults to 5.
            plot (bool, optional): If True matrices are plotted. Defaults to False.
        &#34;&#34;&#34;
        def matrix_avg(M, r=1, A=0, B=1): 
            M_sum = np.full(M.shape, 0.0)
           
            for i in range(M.shape[0]):
                for j in range(M.shape[1]):
                    n0 = n1 = r
                        
                    if (i-n0) &lt;= -1: 
                        n0 = i
                        
                    if (j-n1) &lt;= -1: 
                        n1 = j  
                
                    Mi = M[i-n0:i+r+1, j-n1:j+r+1]
                
                    M_sum[i,j] = np.sum(Mi) / Mi.size
            M_sum = np.interp(M_sum, (M_sum.min(), M_sum.max()), (A, B))
            return M_sum 
        
        if (X is None):
            X = self.X_train
        
        if (y is None):
            y = self.y_train
        
        xy = [X[y == label] for label in np.unique(y)]
        
        x, y = [], []
        for i in range(len(xy)):
            x.append(xy[i][:, 0])
            y.append(xy[i][:, 1])

        x_con, y_con = np.concatenate(x), np.concatenate(y)
        lim = [[min(x_con), max(x_con)], [min(y_con), max(y_con)]]

        data_matrix = []
            
        for i in range(len(xy)):
            data_matrix_i, xaxis, yaxis = np.histogram2d(x[i], y[i], bins=bins, range=lim)
            data_matrix_i = matrix_avg(data_matrix_i, r=r)
            data_matrix.append(data_matrix_i)
        
        background_matrix = np.minimum.reduce(data_matrix)
        
        if plot == True:
            plt.matshow(background_matrix, cmap=plt.cm.viridis)
            plt.show()
            
        weights = np.empty(0)
        for i in range(len(xy)):
            data_matrix[i] = data_matrix[i] - background_matrix
            if plot == True:
                plt.matshow(data_matrix[i], cmap=plt.cm.viridis)
                plt.show()
                
            weights_i = np.empty(0)
            for j in range(len(xy[i])):
                    index_x = np.absolute(xaxis - xy[i][j, 0]).argmin()
                    index_y = np.absolute(yaxis - xy[i][j, 1]).argmin()

                    weights_i = np.append(weights_i, data_matrix[i][index_x - 1, index_y - 1])

            weights = np.hstack([weights, weights_i])
        self.weights = np.interp(weights, (weights.min(), weights.max()), (A, B))

    def export_classifier(self, filePath=None, fileName=None, fileSave=&#39;all&#39;, overwrite=True):
        &#34;&#34;&#34;A function to export the classifier after being fitted or defined. It uses the pickle format, and prints a statement if successfully saved: &#34;Saved the pickle&#34;.

        Args:
            filePath (string, optional): The file path to safe to pickle file in. If None the datafile path is used. Defaults to None.
            fileName (string, optional): The filename of the exported file. If None the datafilename is used, but as a .pickle file. Defaults to None.
            fileSave (string, optional): The part to save. If &#39;all&#39; the entire self saved. Defaults to &#39;all&#39;.
            overwrite (bool, optional): If False And a pickle file with the same name is in the folder a counter (_000) is added to the name. . Defaults to True.
        &#34;&#34;&#34;
        import pickle
        import os.path

        # save type
        if fileSave == &#39;classifier&#39;:
            fileSave = self.cv_search
            
        elif fileSave == &#39;all&#39; or fileSave == None:
            fileSave = self

        else:
            fileSave = self.fileSave
        
        # save path
        if filePath == None:
            filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
            
        if filePath.endswith(&#39;/&#39;) == False:
            filePath += &#39;/&#39;
    
        # save name
        if fileName == None:
            fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)
    
        # combine
        file = filePath + fileName
        
        # adds number
        if overwrite == False:
            i = 0
            file_temp = file
            while os.path.exists(file + &#39;.pickle&#39;) == True:
                file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
                i += 1
            
        with open(filePath + fileName + &#39;.pickle&#39;, &#39;wb&#39;) as handle:
            pickle.dump(fileSave, handle, protocol=pickle.HIGHEST_PROTOCOL)
        
        print(&#39;Saved the pickle&#39;)
    
    def import_classifier(self, filePath=None):
        &#34;&#34;&#34;A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: &#34;Got your pickle!&#34;. It&#39;s not successful: &#34;Unable to locate your pickle&#34;.

        Args:
            filePath (string, optional): The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.

        Returns:
            Classifier (self): The classifier object
        &#34;&#34;&#34;
        import pickle
        import os.path
        
        try: 
            filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
            fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)
            
            if filePath.endswith(&#39;/&#39;) == False:
                filePath += &#39;/&#39;
            
            # combine
            file = filePath + fileName
            
            i = 999
            file_temp = file
            while os.path.exists(file + &#39;.pickle&#39;) == False and i&gt;0:
                
                file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
                i -= 1
            
        
            with open(file + &#39;.pickle&#39;, &#39;rb&#39;) as handle:
                classifier = pickle.load(handle)
            
            print(&#39;Got your pickle!&#39;)
            return classifier
        
        except:
            print(&#39;Unable to locate your pickle&#39;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="quantum_fitter.readout_tools.fitting.Fitting" href="fitting.html#quantum_fitter.readout_tools.fitting.Fitting">Fitting</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="quantum_fitter.readout_tools.loading.DataImport.export_classifier"><code class="name flex">
<span>def <span class="ident">export_classifier</span></span>(<span>self, filePath=None, fileName=None, fileSave='all', overwrite=True)</span>
</code></dt>
<dd>
<div class="desc"><p>A function to export the classifier after being fitted or defined. It uses the pickle format, and prints a statement if successfully saved: "Saved the pickle".</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filePath</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The file path to safe to pickle file in. If None the datafile path is used. Defaults to None.</dd>
<dt><strong><code>fileName</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The filename of the exported file. If None the datafilename is used, but as a .pickle file. Defaults to None.</dd>
<dt><strong><code>fileSave</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The part to save. If 'all' the entire self saved. Defaults to 'all'.</dd>
<dt><strong><code>overwrite</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If False And a pickle file with the same name is in the folder a counter (_000) is added to the name. . Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_classifier(self, filePath=None, fileName=None, fileSave=&#39;all&#39;, overwrite=True):
    &#34;&#34;&#34;A function to export the classifier after being fitted or defined. It uses the pickle format, and prints a statement if successfully saved: &#34;Saved the pickle&#34;.

    Args:
        filePath (string, optional): The file path to safe to pickle file in. If None the datafile path is used. Defaults to None.
        fileName (string, optional): The filename of the exported file. If None the datafilename is used, but as a .pickle file. Defaults to None.
        fileSave (string, optional): The part to save. If &#39;all&#39; the entire self saved. Defaults to &#39;all&#39;.
        overwrite (bool, optional): If False And a pickle file with the same name is in the folder a counter (_000) is added to the name. . Defaults to True.
    &#34;&#34;&#34;
    import pickle
    import os.path

    # save type
    if fileSave == &#39;classifier&#39;:
        fileSave = self.cv_search
        
    elif fileSave == &#39;all&#39; or fileSave == None:
        fileSave = self

    else:
        fileSave = self.fileSave
    
    # save path
    if filePath == None:
        filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
        
    if filePath.endswith(&#39;/&#39;) == False:
        filePath += &#39;/&#39;

    # save name
    if fileName == None:
        fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)

    # combine
    file = filePath + fileName
    
    # adds number
    if overwrite == False:
        i = 0
        file_temp = file
        while os.path.exists(file + &#39;.pickle&#39;) == True:
            file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
            i += 1
        
    with open(filePath + fileName + &#39;.pickle&#39;, &#39;wb&#39;) as handle:
        pickle.dump(fileSave, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
    print(&#39;Saved the pickle&#39;)</code></pre>
</details>
</dd>
<dt id="quantum_fitter.readout_tools.loading.DataImport.import_classifier"><code class="name flex">
<span>def <span class="ident">import_classifier</span></span>(<span>self, filePath=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: "Got your pickle!". It's not successful: "Unable to locate your pickle".</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filePath</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Classifier (self): The classifier object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_classifier(self, filePath=None):
    &#34;&#34;&#34;A function to import the classifier. It uses the pickle format, and prints a statement if successfully imported: &#34;Got your pickle!&#34;. It&#39;s not successful: &#34;Unable to locate your pickle&#34;.

    Args:
        filePath (string, optional): The file path for the file to be imported. If None the folder of the data is being searched. Defaults to None.

    Returns:
        Classifier (self): The classifier object
    &#34;&#34;&#34;
    import pickle
    import os.path
    
    try: 
        filePath = self._get_file_name_from_path(self._filePath, part=&#39;head&#39;)
        fileName = self._get_file_name_from_path(self._filePath).replace(&#39;.hdf5&#39;,&#39;&#39;)
        
        if filePath.endswith(&#39;/&#39;) == False:
            filePath += &#39;/&#39;
        
        # combine
        file = filePath + fileName
        
        i = 999
        file_temp = file
        while os.path.exists(file + &#39;.pickle&#39;) == False and i&gt;0:
            
            file = file_temp + f&#39;_{str(i).zfill(3)}&#39;
            i -= 1
        
    
        with open(file + &#39;.pickle&#39;, &#39;rb&#39;) as handle:
            classifier = pickle.load(handle)
        
        print(&#39;Got your pickle!&#39;)
        return classifier
    
    except:
        print(&#39;Unable to locate your pickle&#39;)</code></pre>
</details>
</dd>
<dt id="quantum_fitter.readout_tools.loading.DataImport.set_data"><code class="name flex">
<span>def <span class="ident">set_data</span></span>(<span>self, channelName=None, state_entries=None, channelName_log='Pulse Generator - Amplitude', unit_log='V')</span>
</code></dt>
<dd>
<div class="desc"><p>Selects the data set which is gonna be used in the further calculations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>channelName</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.</dd>
<dt><strong><code>state_entries</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.</dd>
<dt><strong><code>channelName_log</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The units scale to be used instead of index. Defaults to 'Pulse Generator - Amplitude'.</dd>
<dt><strong><code>unit_log</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The units of the unit scale. Defaults to 'V'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_data(self, channelName=None, state_entries=None, channelName_log=&#39;Pulse Generator - Amplitude&#39;, unit_log=&#39;V&#39;):
    &#34;&#34;&#34;Selects the data set which is gonna be used in the further calculations.

    Args:
        channelName (string, optional): Channel name contained IQ data. If None the first trace in the file is used. Defaults to None.
        state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
        channelName_log (str, optional): The units scale to be used instead of index. Defaults to &#39;Pulse Generator - Amplitude&#39;.
        unit_log (str, optional): The units of the unit scale. Defaults to &#39;V&#39;.
    &#34;&#34;&#34;
    import h5py
    
    h5data = self.h5file.getData(channelName, entry=state_entries)
    self.h5data = self._reformate(h5data)
    
    self.h5data_index = {&#39;name&#39; : &#39;Index&#39;, 
                         &#39;axis&#39; : range(self.h5file.getNumberOfEntries())}
    
    try:
        h5data_temp = h5py.File(self._filePath, &#39;r&#39;)
        h5datalog = h5data_temp[&#39;Data&#39;][&#39;Data&#39;][:]
        self.h5data_log = {&#39;name&#39; : f&#39;{channelName_log} [{unit_log}]&#39;, 
                           &#39;axis&#39; : h5datalog[:,0,0]}
    except:
        print(&#39;channelName_log not in h5data. Setting Pulse Generator Amplitude to index&#39;)
        self.h5data_log = self.h5data_index</code></pre>
</details>
</dd>
<dt id="quantum_fitter.readout_tools.loading.DataImport.set_dataset_size"><code class="name flex">
<span>def <span class="ident">set_dataset_size</span></span>(<span>self, size=None, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the size of all entries in the data set.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The size of the data set used. Must be integer. Defaults to None.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list of data. If None the selected initial states are used. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List</code></dt>
<dd>Dataset shortened by size. (X[:size])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_dataset_size(self, size=None, X=None):
    &#34;&#34;&#34;Sets the size of all entries in the data set.

    Args:
        size (int, optional): The size of the data set used. Must be integer. Defaults to None.
        X (list, optional): A list of data. If None the selected initial states are used. Defaults to None.

    Returns:
        List: Dataset shortened by size. (X[:size])
    &#34;&#34;&#34;
    if size:
        self.size = int(size)
     
    X_temp = False    
    if X is None:
        X_temp = True
        X = self._int_states
        
    state_list, label_list  = [], []
    for i, j in enumerate(X):
        state_list.append(j[:self.size])
        if X_temp == True: 
            label_list.append(np.ones(state_list[i].shape[0]) * self._states_labels[i])
        
    if X_temp == True:  
        self._states_X = np.concatenate(state_list)
        self._states_target = np.concatenate(label_list)
    else:
        return state_list

    self._split_data()</code></pre>
</details>
</dd>
<dt id="quantum_fitter.readout_tools.loading.DataImport.set_states"><code class="name flex">
<span>def <span class="ident">set_states</span></span>(<span>self, state_entries=None, labels=None, offset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the entries of the "cleanest" states of the data set to be used in further calculations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state_entries</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.</dd>
</dl>
<p>offset (float), optional): The offset value offsets the last state in the array. This argument is added to simulate three state data. Defaults to None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_states(self, state_entries=None, labels=None, offset=None):
    &#34;&#34;&#34;Sets the entries of the &#34;cleanest&#34; states of the data set to be used in further calculations.

    Args:
        state_entries (list, optional): A list containing the wanted entries for classification. If None the two entries with the largest and smallest mean are used. Defaults to None.
        labels (list, optional): A list containing the labels for the states. Labels can be integers or a strings. If None numbers from 0 to len(number of states) is used. Defaults to None.
        offset (float), optional): The offset value offsets the last state in the array. This argument is added to simulate three state data. Defaults to None.
    &#34;&#34;&#34;
    if state_entries:
        self.state_entries = state_entries
    
    self._int_states = self.h5data[np.array(self.state_entries)]
    
    if offset:
        self._int_states[-1] = self._int_states[-1] + offset
    
    if labels != None: 
            self._states_labels = labels 
    else: 
        self._states_labels = range(len(self.state_entries))
    
    self.set_dataset_size()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Readout Tools Home">
<img src="https://cdn-icons-png.flaticon.com/512/2432/2432797.png" alt=""> Readout Tools
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="quantum_fitter.readout_tools" href="index.html">quantum_fitter.readout_tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="quantum_fitter.readout_tools.loading.import_classifier" href="#quantum_fitter.readout_tools.loading.import_classifier">import_classifier</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="quantum_fitter.readout_tools.loading.DataImport" href="#quantum_fitter.readout_tools.loading.DataImport">DataImport</a></code></h4>
<ul class="">
<li><code><a title="quantum_fitter.readout_tools.loading.DataImport.export_classifier" href="#quantum_fitter.readout_tools.loading.DataImport.export_classifier">export_classifier</a></code></li>
<li><code><a title="quantum_fitter.readout_tools.loading.DataImport.import_classifier" href="#quantum_fitter.readout_tools.loading.DataImport.import_classifier">import_classifier</a></code></li>
<li><code><a title="quantum_fitter.readout_tools.loading.DataImport.set_data" href="#quantum_fitter.readout_tools.loading.DataImport.set_data">set_data</a></code></li>
<li><code><a title="quantum_fitter.readout_tools.loading.DataImport.set_dataset_size" href="#quantum_fitter.readout_tools.loading.DataImport.set_dataset_size">set_dataset_size</a></code></li>
<li><code><a title="quantum_fitter.readout_tools.loading.DataImport.set_states" href="#quantum_fitter.readout_tools.loading.DataImport.set_states">set_states</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>