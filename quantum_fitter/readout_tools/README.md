# Readout discimination tools
<p align="center">
  <img src="https://img.shields.io/static/v1?style=for-the-badge&label=code-status&message=Caution!&color=red"/>
  <img src="https://img.shields.io/static/v1?style=for-the-badge&label=initial-commit&message=Malthe&color=inactive"/>
  <img src="https://img.shields.io/static/v1?style=for-the-badge&label=maintainer&message=Malthe&color=inactive"/>
</p>
This code is the initial attempt of making a readout discrimination tool using main SVM. 
Other types of machine learning can still be implemented in the code.

Different kinds of SVM kernels are used and tested. The kernels are as follows:
* [linear](https://scikit-learn.org/stable/auto_examples/svm/plot_linearsvc_support_vectors.html): A linear classifier.
* [poly](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): A polynomial classifier.
* [rbf](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html): A Radial Basis Function (RBF) kernel SVM.
* [sigmoid](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): A Hyperbolic Tangent Kernel.
* [(precomputed)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): Haven't looked at this yet.

The code mainly uses sklearn's SVM module, but the follwing types of classifiers can be 
implemented directly in the code:
* [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html): A classifier using the K number of neighbors.
* [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html): A meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies.
* [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html): A classic decision tree classifier.
* [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html): A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule.

# Overall Structure
The code follows a simple class-based structure, where all definitions are made in /Class_setup.py. 
It can import data from Labber files and label these according to the user. 
Furthermore it can generate adecision function for a 2D SVC. Using the 'matrix_wieghts' function the 
code can estimate weights for each datapoint. Lastly the code can plot the data used to generate the 
discriminator, it can plot a testing sample using the classifier and it can generate a ROC plot showing 
the Efficiency of the classifier.

### Overview of the functionalities of the definitions:

**get_labber_data** simply loads the labber filepath `file` with a specific sourece `source` (I.e. 'Pulse Generator - Single-shot, QB1').
It returns `x_array` (array of array).
```python
    get_labber_data(cls, file, source)
```


**combine_and_label** adds two (or more) arrays and label these accordingly. See example SVM_Testing.py line 8.
It return `X` (n times 2 array of X.real, X.imag), `y` (labels).
```python
    combine_and_label(cls, x_array, X_list, label_list=None, n_points=None)
```


**plot_classifier_decision_function** creates grid to evaluate classifier. Takes the type of classifier to use `classifiers`, 
the resolution of the linespace `resolution`, the ax for the figure `ax` (if `ax=None` then it creates a new `ax`), and `plot_support` 
adds errorlines (dotted) to figure (These are not available for all classifiers). 
```python
    plot_classifier_decision_function(cls, classifiers, resolution=100, ax=None, plot_support=True)
```

**matrix_wieghts** generates weights for all datapoints, by generating matrices of the two datasets used for discimination.
It takes the data `X, y`, normalizes the weights in the range `[A,B]` from matrices for the size `[bins,bins]`.
Returns array of `weights`.
IMPORTANT: _This is not backed up by any articles I've been able to find. This will get updated._
```python
    matrix_wieghts(cls, X, y, A=0, B=1, bins=27)
```

**plot_classifier** plots the `classifier` along with the data `X, y`. `sample_weight` is optional and does only 
change the looks of the plot. Returns a plot.
```python
    plot_classifier(cls, X, y, classifiers, sample_weight=1)
```

**plot_testing** uses an already calculated `classifier` to test the discimination tool on a new dataset `X`.
Returns a plot, a array of labeled datapoints, and a `average`.
```python
    plot_testing(cls, X, classifiers)
```

**plot_ROC** generates a ROC plot of the performance of the `classifier` from data `X, y`.
Returns a ROC plot, with a score. 
```python
    plot_ROC(cls, X, y, classifiers)
```

# Demonstration example:
An code example of how to use the code can be found under /SVM_Testing.py. 
Underneath we will walk through the example. To use the code, do the following steps:

#### Step 1: Import the your data:
```python
# Importing the data
path = 'example_data/Data2/ss_q1_rabi_v_ampl_5_fine.hdf5'
X_array = SSD.get_labber_data(path, 'Pulse Generator - Single-shot, QB1')
```

Then select two (or more) states, and label them:
```python
# selecting and importing data
X, y = SSD.combine_and_label(X_array, (40, 80), (0, 1), 1000)
X_train, X_test, y_train, y_test = SSD.train_test_split(X, y, test_size=0.5, random_state=0)
```

#### Step 2: Train and plot the classifier:
```python
# plotting and testing classifier
SSD.plot_classifier(X_train, y_train, classifier, sample_weight=wieghts)
SSD.plot_ROC(X_test, y_test, classifier)
```

#### Step 3: Use classifier to determine states in other data:
```python
#running a test on dataset 75
X_test1, y_test1 = SSD.combine_and_label(X_array, [75], [1], 2000)
SSD.plot_testing(X_test1, classifier)
```